{"cells":[{"metadata":{},"cell_type":"markdown","source":["# **1. Deep Long Short Term Memory Networks [Binary Classifier]:**\n","## **Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data.**"]},{"metadata":{},"cell_type":"markdown","source":["## **Libraries/Dependencies**"]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["# Import all the required libraries\n","# Use Kaggle's pre-tuned notebooks to get the optimal versions of all the dependencies\n","\n","import nltk\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","# nltk.download('stopwords')\n","from string import punctuation\n","from collections import Counter\n","from nltk.corpus import stopwords\n","from torch.utils.data import TensorDataset, DataLoader"],"execution_count":80,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## **Load Dataset & Initialize GPU**"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Load the transfer learning tweet dataset\n","sentiment_df = pd.read_csv('../input/twitterdata/finalSentimentdata2.csv')"],"execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Checking if NVIDIA Graphics Card and CUDA is available\n","gpu_available = torch.cuda.is_available\n","\n","if gpu_available:\n","    print('Parallely Processing using CUDA')\n","else:\n","    print('No CUDA Detected')"],"execution_count":82,"outputs":[{"output_type":"stream","text":"Parallely Processing using CUDA\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["## **Pre-processing & Inference Module Definitions**"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Pre-process the text and perform Stemming, Lemmatization and Stop-word removal\n","def text_preprocessing(text):\n","    remove_punctuation = [ch for ch in text if ch not in punctuation]\n","    remove_punctuation = \"\".join(remove_punctuation).split()\n","    filtered_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n","    return filtered_text\n","\n","\n","# Pad blank topken to keep the length of tweets consistent - mandatory to normalize and train the model\n","def pad_features(reviews_int, seq_length):\n","    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n","    for i, row in enumerate(reviews_int):\n","        if len(row)!=0:\n","            features[i, -len(row):] = np.array(row)[:seq_length]\n","    return features\n","\n","# Convert the sentences into stream of tokens\n","def tokenize(tweet):\n","    test_ints = []\n","    test_ints.append([vocab_to_int[word] for word in tweet])\n","    return test_ints\n","\n","# Predict the sentiment of the tweet - performs binary classification using the model inference\n","def sentiment(net, test_tweet, seq_length=50):\n","    print(\"\\n--------------------------------------------------------------------------------------\")\n","    print(f\"\\n Original input sentence: {test_tweet}\")\n","    test_tweet = text_preprocessing(test_tweet)\n","    tokenized_tweet = tokenize(test_tweet)\n","    \n","    print(f\"\\n Pre-processed input sentence: {test_tweet}\")\n","    #print(f\"\\nSentence converted into tokens:\\n{tokenized_tweet}\")\n","    \n","    padded_tweet = pad_features(tokenized_tweet, 50)\n","    feature_tensor = torch.from_numpy(padded_tweet)\n","    batch_size = feature_tensor.size(0)\n","    \n","    if gpu_available:\n","        feature_tensor = feature_tensor.cuda()\n","    \n","    h = net.init_hidden(batch_size)\n","    output, h = net(feature_tensor, h)\n","    \n","    predicted_sentiment = torch.round(output.squeeze())\n","    \n","    if predicted_sentiment == 1:\n","        print(\"\\n Sentiment: Positive\")\n","        \n","    else:\n","        print(\"\\n Sentiment: Negative\")"],"execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Code block to invoke Pre-processing, Padding and Tokenization operations on the tweet\n","\n","sentiment_df.loc[:, 'text'] = sentiment_df['text'].apply(text_preprocessing)\n","\n","reviews_split = []\n","for i, j in sentiment_df.iterrows():\n","    reviews_split.append(j['text'])\n","\n","words = []\n","for review in reviews_split:\n","    for word in review:\n","        words.append(word)\n","\n","counts = Counter(words)\n","vocab = sorted(counts, key=counts.get, reverse=True)\n","vocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n","\n","encoded_reviews = []\n","for review in reviews_split:\n","    encoded_reviews.append([vocab_to_int[word] for word in review])\n","\n","labels_to_int = []\n","for i, j in sentiment_df.iterrows():\n","    if j['sentiment']=='joy':\n","        labels_to_int.append(1)\n","    else:\n","        labels_to_int.append(0)\n","\n","reviews_len = Counter([len(x) for x in encoded_reviews])\n","non_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(encoded_reviews)!=0]\n","encoded_reviews = [encoded_reviews[ii] for ii in non_zero_idx]\n","encoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])\n","\n","seq_length = 50\n","padded_features= pad_features(encoded_reviews, seq_length)"],"execution_count":84,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## **Dataset and Dataloaders for Train, Test and Validation**"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Split the dataset into Train (80%), Validation (10%) & Test (10%)\n","batch_size = 1\n","split_frac = 0.8\n","split_idx = int(len(padded_features)*split_frac)\n","\n","training_x, remaining_x = padded_features[:split_idx], padded_features[split_idx:]\n","training_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n","\n","test_idx = int(len(remaining_x)*0.5)\n","val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n","val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n","\n","# Transform the data into a Tensor datastructure\n","train_data = TensorDataset(torch.from_numpy(training_x), torch.from_numpy(training_y))\n","test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n","valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n","\n","# Prepare the dataloader for Train, Test and Validation\n","train_loader = DataLoader(train_data, batch_size=batch_size)\n","test_loader = DataLoader(test_data, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, batch_size=batch_size)"],"execution_count":85,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## **LSTM Model Architecture**"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Embedding Dimension of Tokens\n","embedding_dim = 400\n","\n","# Embedding Dimension of Hidden Layers\n","hidden_dim = 256\n","\n","# Output of the model is binary (either Positive or Negative)\n","output_size = 1\n","\n","# Number of hidden LSTM cells\n","n_layers = 2\n","vocab_size = len(vocab_to_int)+1"],"execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Structure of the Neural Network\n","class LSTM(nn.Module):\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n","        super(LSTM, self).__init__()\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        self.dropout = nn.Dropout(0.3)\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        self.sig = nn.Sigmoid()\n","    \n","    def forward(self, x, hidden):\n","        batch_size = x.size(0)\n","        x = x.long()\n","        embeds = self.embedding_layer(x)\n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        sig_out = self.sig(out)\n","        sig_out = sig_out.view(batch_size, -1)\n","        sig_out = sig_out[:, -1]\n","        return sig_out, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        weights = next(self.parameters()).data\n","        if gpu_available:\n","            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n","        return hidden\n","\n","net = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)"],"execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Hyperparameters required for training of the network\n","\n","# Learning Rate\n","lr = 0.001\n","\n","# Loss Function - Binary Cross Entropy\n","criterion = nn.BCELoss()\n","\n","# Gradient Descent based Optimizer - ADAM (Adaptive LR)\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","\n","# Number of epochs to train the model\n","epochs = 8\n","count = 0\n","\n","# Step size\n","print_every = 200\n","clip = 5 "],"execution_count":88,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## **Model Training**"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Train the Neural Network\n","# Off-load the model to CUDA\n","if gpu_available:\n","    net.cuda()\n","\n","net.train()\n","for e in range(epochs):\n","    h = net.init_hidden(batch_size)\n","    \n","    for inputs, labels in train_loader:\n","        count += 1\n","        \n","        if gpu_available:\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","        h = tuple([each.data for each in h])\n","        \n","        net.zero_grad()\n","        outputs, h = net(inputs, h)\n","        loss = criterion(outputs.squeeze(), labels.float())\n","        \n","        loss.backward()\n","        nn.utils.clip_grad_norm(net.parameters(), clip)\n","        optimizer.step()\n","        \n","        if count % print_every == 0:\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            \n","            for inputs, labels in valid_loader:\n","                val_h = tuple([each.data for each in val_h])\n","                \n","                if gpu_available:\n","                    inputs, labels = inputs.cuda(), labels.cuda()\n","                    \n","            outputs, val_h = net(inputs, val_h)\n","            val_loss = criterion(outputs.squeeze(), labels.float())\n","            val_losses.append(val_loss.item())\n","        \n","            net.train()\n","            print(f\"Epoch: {e+1}/{epochs}.....\",f\"Step: {count}.....\",\"Train Loss: {:.6f}......\".format(loss.item()),\"Validation Loss: {:.6f}\".format(np.mean(val_losses)))"],"execution_count":89,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1/8..... Step: 200..... Train Loss: 0.052577...... Validation Loss: 0.081068\nEpoch: 1/8..... Step: 400..... Train Loss: 0.051915...... Validation Loss: 0.046640\nEpoch: 1/8..... Step: 600..... Train Loss: 0.067061...... Validation Loss: 0.061379\nEpoch: 1/8..... Step: 800..... Train Loss: 0.539269...... Validation Loss: 0.763887\nEpoch: 1/8..... Step: 1000..... Train Loss: 0.020809...... Validation Loss: 0.022876\nEpoch: 1/8..... Step: 1200..... Train Loss: 0.733645...... Validation Loss: 0.086998\nEpoch: 1/8..... Step: 1400..... Train Loss: 0.051352...... Validation Loss: 0.053301\nEpoch: 1/8..... Step: 1600..... Train Loss: 0.058821...... Validation Loss: 0.085194\nEpoch: 1/8..... Step: 1800..... Train Loss: 0.073794...... Validation Loss: 0.138465\nEpoch: 1/8..... Step: 2000..... Train Loss: 0.119580...... Validation Loss: 0.060050\nEpoch: 1/8..... Step: 2200..... Train Loss: 0.189531...... Validation Loss: 0.220033\nEpoch: 1/8..... Step: 2400..... Train Loss: 0.071552...... Validation Loss: 0.102625\nEpoch: 2/8..... Step: 2600..... Train Loss: 0.040561...... Validation Loss: 0.383405\nEpoch: 2/8..... Step: 2800..... Train Loss: 0.290448...... Validation Loss: 0.042435\nEpoch: 2/8..... Step: 3000..... Train Loss: 0.007935...... Validation Loss: 0.111118\nEpoch: 2/8..... Step: 3200..... Train Loss: 0.001488...... Validation Loss: 0.016309\nEpoch: 2/8..... Step: 3400..... Train Loss: 0.654472...... Validation Loss: 0.232418\nEpoch: 2/8..... Step: 3600..... Train Loss: 0.059732...... Validation Loss: 0.132370\nEpoch: 2/8..... Step: 3800..... Train Loss: 0.029144...... Validation Loss: 0.245997\nEpoch: 2/8..... Step: 4000..... Train Loss: 0.004949...... Validation Loss: 0.061721\nEpoch: 2/8..... Step: 4200..... Train Loss: 0.015506...... Validation Loss: 0.460073\nEpoch: 2/8..... Step: 4400..... Train Loss: 0.012345...... Validation Loss: 0.017140\nEpoch: 2/8..... Step: 4600..... Train Loss: 0.026798...... Validation Loss: 0.089524\nEpoch: 2/8..... Step: 4800..... Train Loss: 0.017867...... Validation Loss: 0.023573\nEpoch: 3/8..... Step: 5000..... Train Loss: 0.002245...... Validation Loss: 0.072507\nEpoch: 3/8..... Step: 5200..... Train Loss: 0.002492...... Validation Loss: 0.009708\nEpoch: 3/8..... Step: 5400..... Train Loss: 0.003892...... Validation Loss: 0.012431\nEpoch: 3/8..... Step: 5600..... Train Loss: 0.001605...... Validation Loss: 0.006622\nEpoch: 3/8..... Step: 5800..... Train Loss: 0.000163...... Validation Loss: 0.001682\nEpoch: 3/8..... Step: 6000..... Train Loss: 0.002421...... Validation Loss: 0.006245\nEpoch: 3/8..... Step: 6200..... Train Loss: 0.048354...... Validation Loss: 0.007302\nEpoch: 3/8..... Step: 6400..... Train Loss: 0.000326...... Validation Loss: 0.021658\nEpoch: 3/8..... Step: 6600..... Train Loss: 0.002563...... Validation Loss: 0.092125\nEpoch: 3/8..... Step: 6800..... Train Loss: 0.000499...... Validation Loss: 0.006919\nEpoch: 3/8..... Step: 7000..... Train Loss: 0.002714...... Validation Loss: 0.021914\nEpoch: 3/8..... Step: 7200..... Train Loss: 0.003987...... Validation Loss: 0.108117\nEpoch: 3/8..... Step: 7400..... Train Loss: 0.005148...... Validation Loss: 0.038239\nEpoch: 4/8..... Step: 7600..... Train Loss: 0.000154...... Validation Loss: 0.016531\nEpoch: 4/8..... Step: 7800..... Train Loss: 0.000174...... Validation Loss: 0.026354\nEpoch: 4/8..... Step: 8000..... Train Loss: 0.000417...... Validation Loss: 0.000850\nEpoch: 4/8..... Step: 8200..... Train Loss: 0.001164...... Validation Loss: 1.143924\nEpoch: 4/8..... Step: 8400..... Train Loss: 0.000087...... Validation Loss: 0.101797\nEpoch: 4/8..... Step: 8600..... Train Loss: 0.000039...... Validation Loss: 0.023820\nEpoch: 4/8..... Step: 8800..... Train Loss: 0.000525...... Validation Loss: 0.000427\nEpoch: 4/8..... Step: 9000..... Train Loss: 0.000018...... Validation Loss: 0.001151\nEpoch: 4/8..... Step: 9200..... Train Loss: 0.000032...... Validation Loss: 0.000564\nEpoch: 4/8..... Step: 9400..... Train Loss: 0.000061...... Validation Loss: 0.000690\nEpoch: 4/8..... Step: 9600..... Train Loss: 0.000375...... Validation Loss: 0.000350\nEpoch: 4/8..... Step: 9800..... Train Loss: 1.123880...... Validation Loss: 0.000230\nEpoch: 5/8..... Step: 10000..... Train Loss: 0.000042...... Validation Loss: 0.000117\nEpoch: 5/8..... Step: 10200..... Train Loss: 0.000066...... Validation Loss: 0.000096\nEpoch: 5/8..... Step: 10400..... Train Loss: 0.000088...... Validation Loss: 0.000144\nEpoch: 5/8..... Step: 10600..... Train Loss: 0.000028...... Validation Loss: 0.000436\nEpoch: 5/8..... Step: 10800..... Train Loss: 0.000087...... Validation Loss: 0.000351\nEpoch: 5/8..... Step: 11000..... Train Loss: 0.000007...... Validation Loss: 0.000939\nEpoch: 5/8..... Step: 11200..... Train Loss: 0.000004...... Validation Loss: 0.000735\nEpoch: 5/8..... Step: 11400..... Train Loss: 0.000070...... Validation Loss: 0.000030\nEpoch: 5/8..... Step: 11600..... Train Loss: 0.000049...... Validation Loss: 0.000025\nEpoch: 5/8..... Step: 11800..... Train Loss: 0.000021...... Validation Loss: 0.000486\nEpoch: 5/8..... Step: 12000..... Train Loss: 0.000002...... Validation Loss: 0.000167\nEpoch: 5/8..... Step: 12200..... Train Loss: 0.000039...... Validation Loss: 0.000068\nEpoch: 6/8..... Step: 12400..... Train Loss: 0.000013...... Validation Loss: 0.000010\nEpoch: 6/8..... Step: 12600..... Train Loss: 0.000025...... Validation Loss: 0.000013\nEpoch: 6/8..... Step: 12800..... Train Loss: 0.000003...... Validation Loss: 0.000012\nEpoch: 6/8..... Step: 13000..... Train Loss: 0.000017...... Validation Loss: 0.000011\nEpoch: 6/8..... Step: 13200..... Train Loss: 0.000012...... Validation Loss: 0.033108\nEpoch: 6/8..... Step: 13400..... Train Loss: 0.000013...... Validation Loss: 0.001160\nEpoch: 6/8..... Step: 13600..... Train Loss: 0.000012...... Validation Loss: 0.000866\nEpoch: 6/8..... Step: 13800..... Train Loss: 0.000008...... Validation Loss: 0.000724\nEpoch: 6/8..... Step: 14000..... Train Loss: 0.000002...... Validation Loss: 0.000122\nEpoch: 6/8..... Step: 14200..... Train Loss: 0.000004...... Validation Loss: 0.000105\nEpoch: 6/8..... Step: 14400..... Train Loss: 0.000001...... Validation Loss: 0.000098\nEpoch: 6/8..... Step: 14600..... Train Loss: 0.000008...... Validation Loss: 0.000073\nEpoch: 6/8..... Step: 14800..... Train Loss: 0.000004...... Validation Loss: 0.000067\nEpoch: 7/8..... Step: 15000..... Train Loss: 0.000008...... Validation Loss: 0.000049\nEpoch: 7/8..... Step: 15200..... Train Loss: 0.000002...... Validation Loss: 0.000018\nEpoch: 7/8..... Step: 15400..... Train Loss: 0.000012...... Validation Loss: 0.000014\nEpoch: 7/8..... Step: 15600..... Train Loss: 0.000002...... Validation Loss: 0.000012\nEpoch: 7/8..... Step: 15800..... Train Loss: 0.000001...... Validation Loss: 0.000011\nEpoch: 7/8..... Step: 16000..... Train Loss: 0.000002...... Validation Loss: 0.000010\nEpoch: 7/8..... Step: 16200..... Train Loss: 0.000001...... Validation Loss: 0.000010\nEpoch: 7/8..... Step: 16400..... Train Loss: 0.000003...... Validation Loss: 0.000009\nEpoch: 7/8..... Step: 16600..... Train Loss: 0.000007...... Validation Loss: 0.000009\nEpoch: 7/8..... Step: 16800..... Train Loss: 0.000001...... Validation Loss: 0.000008\nEpoch: 7/8..... Step: 17000..... Train Loss: 0.000001...... Validation Loss: 0.000008\nEpoch: 7/8..... Step: 17200..... Train Loss: 0.000000...... Validation Loss: 0.000007\nEpoch: 8/8..... Step: 17400..... Train Loss: 0.000005...... Validation Loss: 0.000006\nEpoch: 8/8..... Step: 17600..... Train Loss: 0.000001...... Validation Loss: 0.000006\nEpoch: 8/8..... Step: 17800..... Train Loss: 0.000018...... Validation Loss: 0.000005\nEpoch: 8/8..... Step: 18000..... Train Loss: 0.000001...... Validation Loss: 0.000005\nEpoch: 8/8..... Step: 18200..... Train Loss: 0.000001...... Validation Loss: 0.000004\nEpoch: 8/8..... Step: 18400..... Train Loss: 0.000000...... Validation Loss: 0.000004\nEpoch: 8/8..... Step: 18600..... Train Loss: 0.000000...... Validation Loss: 0.000004\nEpoch: 8/8..... Step: 18800..... Train Loss: 0.000001...... Validation Loss: 0.000003\nEpoch: 8/8..... Step: 19000..... Train Loss: 0.000000...... Validation Loss: 0.000003\nEpoch: 8/8..... Step: 19200..... Train Loss: 0.000000...... Validation Loss: 0.000003\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 8/8..... Step: 19400..... Train Loss: 0.000004...... Validation Loss: 0.000003\nEpoch: 8/8..... Step: 19600..... Train Loss: 0.000000...... Validation Loss: 0.000002\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["## **Model Testing**"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Train the Neural Network\n","test_losses = []\n","num_correct = 0\n","\n","h = net.init_hidden(batch_size)\n","net.eval()\n","\n","for inputs, labels in test_loader:\n","    h = tuple([each.data for each in h])\n","    \n","    if gpu_available:\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","    \n","    outputs, h = net(inputs, h)\n","    test_loss = criterion(outputs.squeeze(), labels.float())\n","    test_losses.append(test_loss.item())\n","    \n","    pred = torch.round(outputs.squeeze())\n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)\n","\n","\n","test_acc = num_correct/len(test_loader.dataset)\n","\n","print(\"Average Test Loss: {:.4f}\".format(np.mean(test_losses)))\n","print(\"Average Test Accuracy: {:.4f}\".format(test_acc))"],"execution_count":90,"outputs":[{"output_type":"stream","text":"Average Test Loss: 1.0874\nAverage Test Accuracy: 0.8770\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["## **Main Program**"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Driver program\n","print(\"Neural Sentiment Analysis of COVID-19 Tweets with Deep LSTM\")\n","print(\"\\n------Available Options------\")\n","print(\"1. Inference on Sample Tweets\")\n","print(\"2. Enter Custom Tweets/Sentences\")\n","print(\"3. Exit\")\n","print(\"\\nPlease select an option from the above:\")\n","\n","while(True):\n","    choice = int(input())\n","\n","    if choice == 1:\n","        sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n","        sentiment(net, sample_1)\n","    \n","        sample_2 = 'I am happy that my family members are safe in this tough times'\n","        sentiment(net, sample_2)\n","    \n","    elif choice == 2:\n","        print(\"\\nPlease enter a sentence/tweet:\")\n","        user_input = input()\n","        sentiment(net, user_input)\n","    \n","    elif choice == 3:\n","        print(\"\\nExiting...\")\n","        break"],"execution_count":91,"outputs":[{"output_type":"stream","text":"Neural Sentiment Analysis of COVID-19 Tweets with Deep LSTM\n\n------Available Options------\n1. Inference on Sample Tweets\n2. Enter Custom Tweets/Sentences\n3. Exit\n\nPlease select an option from the above:\n1\n\n--------------------------------------------------------------------------------------\n\n Original input sentence: Many lost their jobs because of covid and it is highly dangerous\n\n Pre-processed input sentence: ['many', 'lost', 'jobs', 'covid', 'highly', 'dangerous']\n\n Sentiment: Negative\n\n--------------------------------------------------------------------------------------\n\n Original input sentence: I am happy that my family members are safe in this tough times\n\n Pre-processed input sentence: ['happy', 'family', 'members', 'safe', 'tough', 'times']\n\n Sentiment: Positive\n3\n\nExiting...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# **2. BART: Denoising Autoencoder for Pretraining Sequence-to-Sequence Models [Multi-Class Classifier]:**\n","## **BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pre- training schemes. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive di-alogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. In the below task we utilize this pre-trained model for **Zero-shot Classification.****\n","\n","\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Import all the required libraries\n","from transformers import pipeline\n","classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n","\n","#!pip install git+https://github.com/huggingface/transformers\n","#!git clone https://github.com/huggingface/transformers"],"execution_count":92,"outputs":[{"output_type":"stream","text":"Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Create dataframe for final sentiment classification result\n","def createDataFrame(labels, confidence, tweet):\n","    labels = pd.DataFrame({'Labels': labels})\n","    confidence = pd.DataFrame({'Confidence Scores': confidence})\n","    column_values = ['Labels', 'Confidence']\n","    sentiment_scores = pd.concat([labels,confidence], ignore_index=False, axis=1)\n","    print(\"\\n--------------------------------------------------------------------------------------\")\n","    print(f\"\\n Entered input sentence: {tweet}\")\n","    print(\"\\n Sentiment of the tweet (Probability Distribution): \")\n","    print(sentiment_scores.to_string(index=False))\n","    #print(\"--------------------------------------------------------------------------------------\")"],"execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def sentiment_bart(tweet):\n","    labels = []\n","    confidence = []\n","    \n","    # Possible Sentiment Categories\n","    candidate_labels = [\"happy\", \"sad\", \"warn\", \"angry\", \"sorrow\", \"alert\", \"grief\", \"neutral\"]\n","    \n","    # Send the labels and tweets to the classifier pipeline\n","    result = classifier(tweet, candidate_labels)\n","    \n","    # Extract the labels from results dictionary\n","    labels.append(result[\"labels\"])\n","    labels = [item for sublist in labels for item in sublist] # Flatten the list of lists into list\n","    \n","    # Extract the labels from results dictionary\n","    confidence.append(result[\"scores\"])\n","    confidence = [(str(float(item)*100))[:6]+\" %\" for sublist in confidence for item in sublist] # Flatten the list of lists into list\n","\n","    createDataFrame(labels,confidence, tweet)"],"execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Driver program\n","print(\"Neural Sentiment Analysis of COVID-19 Tweets with BART\")\n","print(\"\\n------Available Options------\")\n","print(\"1. Inference on Sample Tweets\")\n","print(\"2. Enter Custom Tweets/Sentences\")\n","print(\"3. Exit\")\n","print(\"\\nPlease select an option from the above:\")\n","\n","while(True):\n","    choice = int(input())\n","\n","    if choice == 1:\n","        sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n","        sentiment_bart(sample_1)\n","    \n","        sample_2 = 'I am happy that my family members are safe in this tough times'\n","        sentiment_bart(sample_2)\n","    \n","    elif choice == 2:\n","        print(\"\\nPlease enter a sentence/tweet:\")\n","        user_input = input()\n","        sentiment_bart(user_input)\n","    \n","    elif choice == 3:\n","        print(\"\\nExiting...\")\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":"Neural Sentiment Analysis of COVID-19 Tweets with BART\n\n------Available Options------\n1. Inference on Sample Tweets\n2. Enter Custom Tweets/Sentences\n3. Exit\n\nPlease select an option from the above:\n1\n\n--------------------------------------------------------------------------------------\n\n Entered input sentence: Many lost their jobs because of covid and it is highly dangerous\n\n Sentiment of the tweet (Probability Distribution): \n  Labels Confidence Scores\n    warn          51.290 %\n   alert          26.023 %\n     sad          12.588 %\n  sorrow          4.8833 %\n   angry          3.1200 %\n   grief          1.2809 %\n neutral          0.5886 %\n   happy          0.2237 %\n\n--------------------------------------------------------------------------------------\n\n Entered input sentence: I am happy that my family members are safe in this tough times\n\n Sentiment of the tweet (Probability Distribution): \n  Labels Confidence Scores\n   happy          75.238 %\n   alert          16.923 %\n    warn          3.8195 %\n neutral          1.7173 %\n   grief          1.4102 %\n  sorrow          0.3785 %\n     sad          0.3457 %\n   angry          0.1673 %\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}